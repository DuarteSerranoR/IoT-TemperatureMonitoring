{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pickle\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "torch.manual_seed(99)\n",
    "\n",
    "from matplotlib import pyplot as plot\n",
    "\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, max_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    output_size: int\n",
    "\n",
    "    hidden_layer_size: int\n",
    "    lstm: nn.LSTM\n",
    "    linear: nn.Linear\n",
    "    hidden_cell: tuple[torch.Tensor, torch.Tensor]\n",
    "    scaler: MinMaxScaler\n",
    "    device: str\n",
    "\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n",
    "                            torch.zeros(1,1,self.hidden_layer_size))\n",
    "\n",
    "        self.scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(self.device)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq) ,1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]\n",
    "    \n",
    "    def predict(self, input):\n",
    "        X = self.scaler.fit_transform(input.reshape(-1, 1))\n",
    "        X = seq = torch.FloatTensor(X).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            self.hidden = (torch.zeros(1, 1, self.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, self.hidden_layer_size))\n",
    "            preds = self(X)\n",
    "            Y = np.array(preds.cpu())\n",
    "        actual_predictions = self.scaler.inverse_transform(Y.reshape(-1, 1))\n",
    "        return actual_predictions\n",
    "\n",
    "    def set_device(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(self.device)\n",
    "        \n",
    "        self.to(self.device)\n",
    "\n",
    "    def train(self, train_data, train_window = 50, epochs = 100):\n",
    "\n",
    "        # Initiate loss function and optimizer\n",
    "\n",
    "        loss_function = nn.MSELoss().to(self.device)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        \n",
    "        # Scale\n",
    "        \n",
    "        train_data_normalized = self.scaler.fit_transform(train_data.reshape(-1, 1))\n",
    "        train_data_normalized = torch.FloatTensor(train_data_normalized).view(-1).to(self.device)\n",
    "\n",
    "        # Create sequences\n",
    "        \n",
    "        train_inout_seq = []\n",
    "        L = len(train_data_normalized)\n",
    "        for i in range(L-train_window):\n",
    "            train_seq = train_data_normalized[i:i+train_window]\n",
    "            # TODO - offset on the start for direct decoding with several (5) of these models giving (5) concurrent predictions?\n",
    "            train_label = train_data_normalized[i+train_window:i+train_window+self.output_size]\n",
    "            train_inout_seq.append((train_seq ,train_label))\n",
    "        \n",
    "        # Train the model\n",
    "\n",
    "        for i in range(epochs):\n",
    "            _ix = -1\n",
    "            for seq, labels in train_inout_seq:\n",
    "                seq = seq.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                if len(labels) != self.output_size:\n",
    "                    continue\n",
    "\n",
    "                _ix +=1\n",
    "                \n",
    "                #if torch.isnan(seq).any().item():\n",
    "                #    print(f\"nan values in seq at {_ix}\")\n",
    "                #    continue\n",
    "\n",
    "                #if pd.isna(labels.item()):\n",
    "                #    print(f\"nan labels at {_ix}\")\n",
    "                #    continue\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                self.hidden_cell = (torch.zeros(1, 1, self.hidden_layer_size).to(self.device),\n",
    "                                torch.zeros(1, 1, self.hidden_layer_size).to(self.device))\n",
    "\n",
    "                y_pred = self(seq)\n",
    "                #if pd.isna(y_pred.item()):\n",
    "                #    print(f\"nan preds at {_ix}\")\n",
    "                #    continue\n",
    "\n",
    "                single_loss = loss_function(y_pred, labels)\n",
    "                #if pd.isna(single_loss.item()):\n",
    "                #    print(f\"nan loss at {_ix}\")\n",
    "                #    raise\n",
    "                single_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if i%25 == 1:\n",
    "                print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n",
    "\n",
    "        # Finalize\n",
    "        #self.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/datasets/historical.data\")\n",
    "df = df.sort_values(by=\"date\",ascending=True)\n",
    "df_inside = df.loc[df[\"label\"] == \"inside\"].drop([\"label\"], axis=1)\n",
    "df_outside = df.loc[df[\"label\"] == \"outside\"].drop([\"label\"], axis=1)\n",
    "inside_arr=np.array(df_inside)\n",
    "outside_arr=np.array(df_outside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(arr: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    total_len,_=arr.shape\n",
    "\n",
    "    train_p = 0.80\n",
    "    test_p = 0.10\n",
    "    ivs_p = 0.10\n",
    "\n",
    "    train_len = round(total_len*train_p)\n",
    "    test_len = round(total_len*test_p)\n",
    "    ivs_len = round(total_len*ivs_p)\n",
    "\n",
    "    model_len = train_len+test_len\n",
    "\n",
    "    total_used_len = model_len+ivs_len\n",
    "\n",
    "    train = arr[:train_len]\n",
    "    test = arr[train_len:model_len]\n",
    "    ivs = arr[model_len:total_used_len]\n",
    "\n",
    "    return train, test, ivs\n",
    "\n",
    "\n",
    "inside_train, inside_test, inside_ivs = split_data(inside_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Wanted Predictions: 5.0\n"
     ]
    }
   ],
   "source": [
    "per_min = (2*60) + 30\n",
    "per_30_mins = per_min / 30\n",
    "\n",
    "print(\"Number of Wanted Predictions: \" + str(per_30_mins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = inside_train[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTM().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 loss: 0.0002483580\n"
     ]
    }
   ],
   "source": [
    "model.train(train_data, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = inside_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23.494268]]\n",
      "23.375\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(test_data[0:49]))\n",
    "print(test_data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23.527887]]\n",
      "23.437\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(test_data[1:50]))\n",
    "print(test_data[51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23.507456]]\n",
      "23.312\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(test_data[2:51]))\n",
    "print(test_data[52])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_window = 50\n",
    "\n",
    "test_inout_seq = []\n",
    "L = len(test_data)\n",
    "for i in range(L-test_window):\n",
    "    train_seq = test_data[i:i+test_window]\n",
    "    train_label = test_data[i+test_window:i+test_window+1]\n",
    "    test_inout_seq.append((train_seq ,train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printRegStatistics(truth, preds):\n",
    "    print(\"The RVE is: \", explained_variance_score(truth, preds))\n",
    "    print(\"The rmse is: \", mean_squared_error(truth, preds, squared=False))\n",
    "    corr, pval = pearsonr(truth, preds)\n",
    "    print(\"The Correlation Score is is: %6.4f (p-value=%e)\\n\"%(corr,pval))\n",
    "    print(\"The Maximum Error is is: \", max_error(truth, preds))\n",
    "    print(\"The Mean Absolute Error is: \", mean_absolute_error(truth, preds))\n",
    "\n",
    "test_X = [ x for x, _ in test_inout_seq ]\n",
    "true_Y = [ y[0] for _, y in test_inout_seq ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = []\n",
    "for X in test_X:\n",
    "    Y = model.predict(X)[0]\n",
    "    pred_Y.append(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RVE is:  0.8978764419591279\n",
      "The rmse is:  0.1934079703350684\n",
      "The Correlation Score is is: 0.9481 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  1.2700949361588663\n",
      "The Mean Absolute Error is:  0.15038657969655386\n"
     ]
    }
   ],
   "source": [
    "printRegStatistics(true_Y, pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23.774408]]\n",
      "23.562\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(np.array([test_data[1]])))\n",
    "print(test_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.625, 23.437, 23.562, ..., 24.0, 24.312, 24.625], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = model.predict(np.array(test_data[0:49]))[0]\n",
    "pred_Y = list(pred_Y)\n",
    "\n",
    "test_X_stream = test_data[50:]\n",
    "_ix = 0\n",
    "for X in test_X_stream:\n",
    "    if _ix == len(test_X_stream) - 1:\n",
    "        break\n",
    "    Y = model.predict(np.array([X]))[0,0]\n",
    "    pred_Y.append(Y)\n",
    "    _ix+=1\n",
    "\n",
    "pred_Y = np.array(pred_Y)\n",
    "true_Y = test_data[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RVE is:  0.9215021944542544\n",
      "The rmse is:  0.3666193784647493\n",
      "The Correlation Score is is: 0.9608 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  1.650580470826835\n",
      "The Mean Absolute Error is:  0.3415378658962491\n"
     ]
    }
   ],
   "source": [
    "printRegStatistics(true_Y, pred_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the best Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "epoch:   0 loss: 0.0001781297\n",
      "RVE 0.9215223812215173 -> hidden_layer_size '50' ; num_layers '1'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0009761230\n",
      "RVE 0.9215358010995972 -> hidden_layer_size '50' ; num_layers '2'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0005194269\n",
      "RVE 0.9214689878168127 -> hidden_layer_size '50' ; num_layers '4'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0001543104\n",
      "RVE 0.9214912420035739 -> hidden_layer_size '100' ; num_layers '1'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0002654023\n",
      "RVE 0.9215121020220935 -> hidden_layer_size '100' ; num_layers '2'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0001428257\n",
      "RVE 0.9215137929952318 -> hidden_layer_size '100' ; num_layers '4'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0002610044\n",
      "RVE 0.9215320601239063 -> hidden_layer_size '150' ; num_layers '1'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0002356943\n",
      "RVE 0.9215061980910825 -> hidden_layer_size '150' ; num_layers '2'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0003015394\n",
      "RVE 0.9215073628998662 -> hidden_layer_size '150' ; num_layers '4'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0002456461\n",
      "RVE 0.921430979552585 -> hidden_layer_size '200' ; num_layers '1'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0002132360\n",
      "RVE 0.9214925174928129 -> hidden_layer_size '200' ; num_layers '2'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0002379379\n",
      "RVE 0.9214948278872813 -> hidden_layer_size '200' ; num_layers '4'\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "truth = test_data[50:]\n",
    "\n",
    "hidden_layer_sizes = [50,100,150,200]\n",
    "nums_layers = [1,2,4]\n",
    "\n",
    "hyper_params = itertools.product(hidden_layer_sizes, nums_layers)\n",
    "\n",
    "for params in hyper_params:\n",
    "\n",
    "    hidden_layer_size, num_layers = params\n",
    "\n",
    "    model = LSTM(output_size=1, hidden_layer_size=hidden_layer_size, num_layers=num_layers).to(device)\n",
    "    model.train(train_data, epochs=1)\n",
    "    #model.eval()\n",
    "\n",
    "    preds = model.predict(np.array(test_data[0:49]))[0]\n",
    "    preds = list(preds)\n",
    "\n",
    "    _ix = 0\n",
    "    for X in test_data[50:]:\n",
    "        if _ix == len(test_data[50:]) - 1:\n",
    "            break\n",
    "        Y = model.predict(np.array([X]))[0,0]\n",
    "        preds.append(Y)\n",
    "        _ix+=1\n",
    "        \n",
    "\n",
    "    RVE = explained_variance_score(truth, preds)\n",
    "\n",
    "    result = {\"RVE\": RVE, \"hidden_layer_size\": hidden_layer_size, \"num_layers\": num_layers}\n",
    "    print(f\"RVE {RVE} -> hidden_layer_size '{hidden_layer_size}' ; num_layers '{num_layers}'\")\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With multiple inputs + outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "epoch:   0 loss: 0.0002685341\n",
      "RVE 0.9214938741789619 -> hidden_layer_size '50' ; num_layers '1'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0002727906\n",
      "RVE 0.9214787378947527 -> hidden_layer_size '50' ; num_layers '2'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0002610883\n",
      "RVE 0.9215093735563208 -> hidden_layer_size '50' ; num_layers '4'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0002999370\n",
      "RVE 0.9214766680656206 -> hidden_layer_size '100' ; num_layers '1'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0002586408\n",
      "RVE 0.9213946255915522 -> hidden_layer_size '100' ; num_layers '2'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0002734783\n",
      "RVE 0.9215010420884162 -> hidden_layer_size '100' ; num_layers '4'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0002559267\n",
      "RVE 0.9215099047368696 -> hidden_layer_size '150' ; num_layers '1'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0002418453\n",
      "RVE 0.9215074634137506 -> hidden_layer_size '150' ; num_layers '2'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0002745653\n",
      "RVE 0.9214963739094709 -> hidden_layer_size '150' ; num_layers '4'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0002724830\n",
      "RVE 0.9214516656042985 -> hidden_layer_size '200' ; num_layers '1'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0003736119\n",
      "RVE 0.9208490162148623 -> hidden_layer_size '200' ; num_layers '2'\n",
      "cuda\n",
      "epoch:   0 loss: 0.0002743296\n",
      "RVE 0.9215044785224547 -> hidden_layer_size '200' ; num_layers '4'\n"
     ]
    }
   ],
   "source": [
    "results_multilabel = []\n",
    "\n",
    "truth = test_data[50:]\n",
    "\n",
    "hidden_layer_sizes = [50,100,150,200]\n",
    "nums_layers = [1,2,4]\n",
    "\n",
    "hyper_params = itertools.product(hidden_layer_sizes, nums_layers)\n",
    "\n",
    "for params in hyper_params:\n",
    "\n",
    "    hidden_layer_size, num_layers = params\n",
    "\n",
    "    model = LSTM(output_size=5, hidden_layer_size=hidden_layer_size, num_layers=num_layers).to(device)\n",
    "    model.train(train_data, epochs=1)\n",
    "    #model.eval()\n",
    "\n",
    "    preds = model.predict(np.array(test_data[0:49]))[0]\n",
    "    preds = list(preds)\n",
    "\n",
    "    _ix = 0\n",
    "    for X in test_data[50:]:\n",
    "        if _ix == len(test_data[50:]) - 1:\n",
    "            break\n",
    "        Y = model.predict(np.array([X]))[0,0]\n",
    "        preds.append(Y)\n",
    "        _ix+=1\n",
    "        \n",
    "\n",
    "    RVE = explained_variance_score(truth, preds)\n",
    "\n",
    "    result = {\"RVE\": RVE, \"hidden_layer_size\": hidden_layer_size, \"num_layers\": num_layers}\n",
    "    print(f\"RVE {RVE} -> hidden_layer_size '{hidden_layer_size}' ; num_layers '{num_layers}'\")\n",
    "    results_multilabel.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "epoch:   1 loss: 0.00307437\n",
      "epoch:  26 loss: 0.00216066\n",
      "epoch:  51 loss: 0.00314611\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rapos\\OneDrive\\Desktop\\IT\\Dev\\Workspace\\IoT_TemperatureMonitoring\\main.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rapos/OneDrive/Desktop/IT/Dev/Workspace/IoT_TemperatureMonitoring/main.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m LSTM(output_size\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, hidden_layer_size\u001b[39m=\u001b[39m\u001b[39m150\u001b[39m, num_layers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rapos/OneDrive/Desktop/IT/Dev/Workspace/IoT_TemperatureMonitoring/main.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#model.train(model_data, epochs=1)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rapos/OneDrive/Desktop/IT/Dev/Workspace/IoT_TemperatureMonitoring/main.ipynb#X50sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain(model_data, epochs\u001b[39m=\u001b[39;49m\u001b[39m76\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rapos/OneDrive/Desktop/IT/Dev/Workspace/IoT_TemperatureMonitoring/main.ipynb#X50sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#model.train(model_data, epochs=200)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rapos/OneDrive/Desktop/IT/Dev/Workspace/IoT_TemperatureMonitoring/main.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m ivs_data \u001b[39m=\u001b[39m inside_ivs[:,\u001b[39m1\u001b[39m]\n",
      "\u001b[1;32mc:\\Users\\rapos\\OneDrive\\Desktop\\IT\\Dev\\Workspace\\IoT_TemperatureMonitoring\\main.ipynb Cell 27\u001b[0m in \u001b[0;36mLSTM.train\u001b[1;34m(self, train_data, train_window, epochs)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/rapos/OneDrive/Desktop/IT/Dev/Workspace/IoT_TemperatureMonitoring/main.ipynb#X50sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     \u001b[39m#if pd.isna(single_loss.item()):\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/rapos/OneDrive/Desktop/IT/Dev/Workspace/IoT_TemperatureMonitoring/main.ipynb#X50sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     \u001b[39m#    print(f\"nan loss at {_ix}\")\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/rapos/OneDrive/Desktop/IT/Dev/Workspace/IoT_TemperatureMonitoring/main.ipynb#X50sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m     \u001b[39m#    raise\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/rapos/OneDrive/Desktop/IT/Dev/Workspace/IoT_TemperatureMonitoring/main.ipynb#X50sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m     single_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/rapos/OneDrive/Desktop/IT/Dev/Workspace/IoT_TemperatureMonitoring/main.ipynb#X50sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/rapos/OneDrive/Desktop/IT/Dev/Workspace/IoT_TemperatureMonitoring/main.ipynb#X50sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39mif\u001b[39;00m i\u001b[39m%\u001b[39m\u001b[39m25\u001b[39m \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/rapos/OneDrive/Desktop/IT/Dev/Workspace/IoT_TemperatureMonitoring/main.ipynb#X50sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m:\u001b[39;00m\u001b[39m3\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m loss: \u001b[39m\u001b[39m{\u001b[39;00msingle_loss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m10.8f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rapos\\miniconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rapos\\miniconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rapos\\miniconda3\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    158\u001b[0m          grads,\n\u001b[0;32m    159\u001b[0m          exp_avgs,\n\u001b[0;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    162\u001b[0m          state_steps,\n\u001b[0;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\rapos\\miniconda3\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m func(params,\n\u001b[0;32m    214\u001b[0m      grads,\n\u001b[0;32m    215\u001b[0m      exp_avgs,\n\u001b[0;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    218\u001b[0m      state_steps,\n\u001b[0;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[1;32mc:\\Users\\rapos\\miniconda3\\lib\\site-packages\\torch\\optim\\adam.py:256\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[39massert\u001b[39;00m param\u001b[39m.\u001b[39mis_cuda \u001b[39mand\u001b[39;00m step_t\u001b[39m.\u001b[39mis_cuda, \u001b[39m\"\u001b[39m\u001b[39mIf capturable=True, params and state_steps must be CUDA tensors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m \u001b[39m# update step\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m step_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    259\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_data = np.concatenate((train_data, test_data), axis=0)\n",
    "\n",
    "model = LSTM(output_size=5, hidden_layer_size=150, num_layers=1).to(device)\n",
    "#model.train(model_data, epochs=1)\n",
    "model.train(model_data, epochs=26)\n",
    "#model.train(model_data, epochs=200)\n",
    "\n",
    "ivs_data = inside_ivs[:,1]\n",
    "\n",
    "truth = ivs_data[50:]\n",
    "\n",
    "preds = model.predict(np.array(ivs_data[0:49]))[0]\n",
    "preds = list(preds)\n",
    "\n",
    "_ix = 0\n",
    "for X in ivs_data[50:]:\n",
    "    if _ix == len(ivs_data[50:]) - 1:\n",
    "        break\n",
    "    Y = model.predict(np.array([X]))[0,0]\n",
    "    preds.append(Y)\n",
    "    _ix+=1\n",
    "\n",
    "printRegStatistics(truth, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./model\",\"wb\") as o:\n",
    "    pickle.dump(model, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(1, 100)\n",
      "  (linear): Linear(in_features=100, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "with open(\"./model\",\"rb\") as o:\n",
    "    model = pickle.load(o, encoding='bytes')\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
